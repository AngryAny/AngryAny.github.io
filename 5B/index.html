<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5B: Training Diffusion Models</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
            color: #333;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
        }
        
        h3 {
            color: #5d6d7e;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        h4 {
            color: #7d8a97;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        .image-container {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }
        
        .image-caption {
            margin-top: 10px;
            font-style: italic;
            color: #666;
            font-size: 14px;
        }
        
        .gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 30px;
        }
        
        .gallery-item {
            background-color: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            text-align: center;
        }
        
        .gallery-item img {
            width: 100%;
            height: auto;
            border-radius: 5px;
        }
        
        .intro {
            background-color: white;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }

        .small-gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .small-gallery-item {
            background-color: white;
            padding: 10px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            text-align: center;
        }

        .small-gallery-item img {
            width: 100%;
            height: auto;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <h1>Project 5B: Training Diffusion Models</h1>
    
    <div class="intro">
        <p>In this project, we train our own diffusion models from scratch on MNIST digits. We start with a simple UNet denoiser, then add time conditioning, and finally class conditioning to generate specific digits.</p>
    </div>

    <h2>Part 1.2: Using the UNet to Train a Denoiser</h2>

    <p>In this section we explore how to use a UNet to denoise MNIST images instead of general images from the diffusion model. We first show how noise is added by visualizing the digit 2 at different noise levels:</p>

    <div class="image-container">
        <img src="media/noising_digit_2.png" alt="Noising process visualization">
        <div class="image-caption">Noising process on digit 2 with sigma = [0, 0.2, 0.4, 0.6, 0.8, 1.0]</div>
    </div>

    <h3>1.2.1 Training</h3>

    <p>We trained the UNet on MNIST with sigma = 0.5. The training loss curve is as follows:</p>

    <div class="image-container">
        <img src="media/training_loss_unet.png" alt="Training loss curve">
        <div class="image-caption">Training Loss Curve for UNet (sigma = 0.5)</div>
    </div>

    <p>As a comparison of how the UNet performs at different points during training, we show the results after the 1st and 5th epochs on a few examples:</p>

    <div class="small-gallery">
        <div class="small-gallery-item">
            <img src="media/denoise_unet_epoch_1.png" alt="Denoising after epoch 1">
            <div class="image-caption">After Epoch 1</div>
        </div>
        <div class="small-gallery-item">
            <img src="media/denoise_unet_epoch_5.png" alt="Denoising after epoch 5">
            <div class="image-caption">After Epoch 5</div>
        </div>
    </div>

    <p>We can see that the UNet performs better after the 5th epoch than the first epoch. This is because the UNet is not able to denoise the image as well after the 1st epoch and still has some extra white traces.</p>

    <h3>1.2.2 Out-of-Distribution Denoising</h3>

    <p>We also tried to denoise images out of distribution by testing various values of sigma from 0 to 1. The results for 3 examples are as follows:</p>

    <div class="image-container">
        <img src="media/ood_noise.png" alt="Out-of-distribution noise results">
        <div class="image-caption">Out-of-Distribution Denoising (varying sigma from 0 to 1)</div>
    </div>

    <p>We can see that the UNet has a higher loss but the training loss seems to be relatively stable.</p>

    <h3>1.2.3 Pure Noise Denoising</h3>

    <p>We tried to denoise pure noise by training the UNet on pure noise. The training loss curve is as follows:</p>

    <div class="image-container">
        <img src="media/pure_noise_loss.png" alt="Pure noise training loss">
        <div class="image-caption">Training Loss Curve for Pure Noise Denoising</div>
    </div>

    <p>The results for pure noise after the 1st and 5th epochs are as follows:</p>

    <div class="small-gallery">
        <div class="small-gallery-item">
            <img src="media/pure_noise_denoise_epoch_1.png" alt="Pure noise epoch 1">
            <div class="image-caption">After Epoch 1</div>
        </div>
        <div class="small-gallery-item">
            <img src="media/pure_noise_denoise_epoch_5.png" alt="Pure noise epoch 5">
            <div class="image-caption">After Epoch 5</div>
        </div>
    </div>

    <p>We see that epoch 5 is a bit less blurry but all of the outputs look the same for a given epoch. I think that because the input does not provide any information at all, the UNet learned to just not use the input and output a constant image that is the closest to the average of the training data (it looks like all of the digits where it is more light when there are more digits going through it).</p>

    <h2>Part 2.2: Time-Conditioned UNet Training</h2>

    <p>We first show the training loss curve for the time-conditioned UNet:</p>

    <div class="image-container">
        <img src="media/training_loss_time.png" alt="Time-conditioned UNet training loss">
        <div class="image-caption">Training Loss Curve for Time-Conditioned UNet</div>
    </div>

    <h2>Part 2.3: Sampling from the Time-Conditioned UNet</h2>

    <p>We trained a time-conditioned UNet to help provide extra context for the denoising process. By having this extra time parameter, it gives the UNet more information about how much noise there is and thus helps it decode. Here are the results after training for 1, 5, and 10 epochs:</p>

    <div class="gallery">
        <div class="gallery-item">
            <img src="media/time_conditional_samples_epoch_001.png" alt="Time-conditioned samples epoch 1">
            <div class="image-caption">Epoch 1</div>
        </div>
        <div class="gallery-item">
            <img src="media/time_conditional_samples_epoch_005.png" alt="Time-conditioned samples epoch 5">
            <div class="image-caption">Epoch 5</div>
        </div>
        <div class="gallery-item">
            <img src="media/time_conditional_samples_epoch_010.png" alt="Time-conditioned samples epoch 10">
            <div class="image-caption">Epoch 10</div>
        </div>
    </div>

    <p>We can see that the time-conditioned UNet struggles to generate digits even after 10 epochs where it has stabilized in training loss. I think this may be due to not knowing which digit to generate since the UNet will likely just generate a "random" looking digit. Note that it is better than the pure noise model as the time parameter provides some context and so it looks less noisy.</p>

    <h2>Part 2.4-2.5: Class-Conditioned UNet Training</h2>

    <p>We now give the model even more context: we tell the UNet the class of the image (what digit it is) and see how it affects performance as well as sampling. If this works, it would let us generate chosen digits from the UNet instead of random digits.</p>

    <p>We first show the training loss curve for the class-conditioned UNet:</p>

    <div class="image-container">
        <img src="media/class_training_loss.png" alt="Class-conditioned UNet training loss">
        <div class="image-caption">Training Loss Curve for Class-Conditioned UNet</div>
    </div>

    <p>We can see that the loss decreases over time, indicating that it does better on the training data.</p>

    <h2>Part 2.6: Sampling from the Class-Conditioned UNet</h2>

    <p>We show the sampling results for the class-conditioned UNet, generating 4 instances of each digit:</p>

    <div class="gallery">
        <div class="gallery-item">
            <img src="media/class_conditional_samples_epoch_001.png" alt="Class-conditioned samples epoch 1">
            <div class="image-caption">Epoch 1</div>
        </div>
        <div class="gallery-item">
            <img src="media/class_conditional_samples_epoch_005.png" alt="Class-conditioned samples epoch 5">
            <div class="image-caption">Epoch 5</div>
        </div>
        <div class="gallery-item">
            <img src="media/class_conditional_samples_epoch_010.png" alt="Class-conditioned samples epoch 10">
            <div class="image-caption">Epoch 10</div>
        </div>
    </div>

    <p>We can see that the class-conditioned UNet is able to generate the digits much earlier into the training than the time-conditioned UNet. It seems that this extra piece of information is enough to help the UNet converge faster.</p>

</body>
</html>
